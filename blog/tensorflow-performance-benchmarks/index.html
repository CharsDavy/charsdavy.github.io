<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
  Jekyll integration by somiibo.com
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
--><html>
	<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

<title>Tensorflow -- 基准</title>
<meta name="description" content="">

<link rel="apple-touch-icon" sizes="180x180" href="/assets/icon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/icon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/icon/favicon-16x16.png">
<link rel="manifest" href="/assets/icon/manifest.json">
<link rel="mask-icon" href="/assets/icon/safari-pinned-tab.svg" color="#5bbad5">
<link rel="shortcut icon" href="/assets/icon/favicon.ico">
<meta name="msapplication-config" content="/assets/icon/browserconfig.xml">
<meta name="theme-color" content="#ffffff">

<!-- CSS -->
<link rel="stylesheet" href="/assets/css/main.css">
<noscript><link rel="stylesheet" href="/assets/css/noscript.css"></noscript>

	</head>
	<body class="is-loading">

		<!-- Wrapper -->
			<div id="wrapper" class="fade-in">

				<!-- Header -->
        <header id="header">
          <a href="/" class="logo">Chars's Tech Blog</a>
        </header>

				<!-- Nav -->
					<nav id="nav">

            <ul class="links">
  <li class=""><a href="/">Home</a></li>
  <li class=" active "><a href="/blog/">Blog</a></li>
  <li class=""><a href="/tags/">Tags</a></li>
  <li class=""><a href="/categories/">Categories</a></li>
  <li class=""><a href="/about/">About</a></li>
<!--   <li class=""><a href="/elements/">Elements Reference</a></li> -->
</ul>


						<ul class="icons">
              <li><a href="https://twitter.com/charsdavy" class="icon fa-twitter" rel="nofollow"><span class="label">Twitter</span></a></li>
              <li><a href="https://facebook.com/wei.deng.1460" class="icon fa-facebook" rel="nofollow"><span class="label">Facebook</span></a></li>
              <li><a href="https://instagram.com/chars.davy" class="icon fa-instagram" rel="nofollow"><span class="label">Instagram</span></a></li>
              <li><a href="https://github.com/charsdavy" class="icon fa-github" rel="nofollow"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
				<div id="main">
          <section class="post">
    				<header class="major">
      				<span class="date">05 Feb 2018</span>
      				<h1>Tensorflow -- 基准</h1>
      				<!-- <p><p>TensorFlow 社区创建了一系列用于多平台测试的图像分类模型参考点。在 <a href="#methodology">方法</a> 章节中会详细说明如何执行测试，并给出使用的脚本链接。</p>

</p> -->
      			</header>
      			<div class="image main"><img src="" alt=""></div>
      			<p></p>
<p>TensorFlow 社区创建了一系列用于多平台测试的图像分类模型参考点。在 <a href="#methodology">方法</a> 章节中会详细说明如何执行测试，并给出使用的脚本链接。</p>

<!-- more -->

<h2 id="图像分类模型的结果">图像分类模型的结果</h2>

<p>InceptionV3 (<a href="https://arxiv.org/abs/1512.00567">arXiv:1512.00567</a>), ResNet-50
(<a href="https://arxiv.org/abs/1512.03385">arXiv:1512.03385</a>), ResNet-152
(<a href="https://arxiv.org/abs/1512.03385">arXiv:1512.03385</a>), VGG16
(<a href="https://arxiv.org/abs/1409.1556">arXiv:1409.1556</a>), 和
<a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a> 使用 <a href="http://www.image-net.org/">ImageNet</a> 数据集测试。这些测试运行在 Google 计算云引擎，亚马逊计算云 (Amazon EC2) 和 NVIDIA® DGX-1™ 。大部分测试使用合成和真实的数据。</p>

<p>对合成数据的测试是通过使用一个 <code class="highlighter-rouge">tf.Variable</code> 设置相同的 shape，除了每个 ImageNet 模型。我们认为，当评估一个平台的基准时包含真实数据是很重要的。底层硬件和框架的加载测试是为了训练实际数据。我们开始合成数据用来移除磁盘 I/O 作为一个变量，并设置一个基准。然后使用真实的数据来验证 TensorFlow 的输入和底层磁盘 I/O 的计算单元。</p>

<h3 id="使用-nvidia-dgx-1-nvidia-tesla-p100-训练">使用 NVIDIA® DGX-1™ (NVIDIA® Tesla® P100) 训练</h3>

<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:80%" src="https://www.tensorflow.org/images/perf_summary_p100_single_server.png">
</div>

<p>细节和附加结果在 <a href="#details_for_nvidia_dgx-1tm_nvidia_tesla_p100">NVIDIA® DGX-1™ (NVIDIA®
Tesla® P100) 的细节</a> 章节中。</p>

<h3 id="使用-nvidia-tesla-k80-训练">使用 NVIDIA® Tesla® K80 训练</h3>

<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:80%" src="https://www.tensorflow.org/images/perf_summary_k80_single_server.png">
</div>

<p>细节和附加结果在 <a href="#details_for_google_compute_engine_nvidia_tesla_k80">Google 计算引擎
(NVIDIA® Tesla® K80) 的细节</a> 和
<a href="#details_for_amazon_ec2_nvidia_tesla_k80">Amazon EC2 (NVIDIA® Tesla®
K80) 的细节</a> 章节中。</p>

<h3 id="使用-nvidia-tesla-k80-分布式训练">使用 NVIDIA® Tesla® K80 分布式训练</h3>

<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:80%" src="https://www.tensorflow.org/images/perf_summary_k80_aws_distributed.png">
</div>

<p>细节和附加结果在 <a href="#details_for_amazon_ec2_distributed_nvidia_tesla_k80">分布式 Amazon EC2
(NVIDIA® Tesla® K80) 的细节</a>
章节中。</p>

<h3 id="比较合成和真实训练数据">比较合成和真实训练数据</h3>

<p><strong>NVIDIA® Tesla® P100</strong></p>

<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:35%" src="https://www.tensorflow.org/images/perf_summary_p100_data_compare_inceptionv3.png">
  <img style="width:35%" src="https://www.tensorflow.org/images/perf_summary_p100_data_compare_resnet50.png">
</div>

<p><strong>NVIDIA® Tesla® K80</strong></p>

<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:35%" src="https://www.tensorflow.org/images/perf_summary_k80_data_compare_inceptionv3.png">
  <img style="width:35%" src="https://www.tensorflow.org/images/perf_summary_k80_data_compare_resnet50.png">
</div>

<h2 id="nvidia-dgx-1-nvidia-tesla-p100-的细节">NVIDIA® DGX-1™ (NVIDIA® Tesla® P100) 的细节</h2>

<h3 id="环境配置">环境配置</h3>

<ul>
  <li>
<strong>Instance type</strong>: NVIDIA® DGX-1™</li>
  <li>
<strong>GPU:</strong> 8x NVIDIA® Tesla® P100</li>
  <li>
<strong>OS:</strong> Ubuntu 16.04 LTS with tests run via Docker</li>
  <li>
<strong>CUDA / cuDNN:</strong> 8.0 / 5.1</li>
  <li>
<strong>TensorFlow GitHub hash:</strong> b1e174e</li>
  <li>
<strong>Benchmark GitHub hash:</strong> 9165a70</li>
  <li>
<strong>Build Command:</strong> <code class="highlighter-rouge">bazel build -c opt --copt=-march="haswell" --config=cuda
//tensorflow/tools/pip_package:build_pip_package</code>
</li>
  <li>
<strong>Disk:</strong> Local SSD</li>
  <li>
<strong>DataSet:</strong> ImageNet</li>
  <li>
<strong>Test Date:</strong> May 2017</li>
</ul>

<p>每个模型的批处理大小和优化器如下表所示。除了下表所示的批处理大小，InceptionV3、ResNet-50、ResNet-152 和 VGG16 也用批处理大小为 32 进行测试。这些结果在 <em>其他结果</em> 章节。</p>

<table>
  <thead>
    <tr>
      <th>Options</th>
      <th>InceptionV3</th>
      <th>ResNet-50</th>
      <th>ResNet-152</th>
      <th>AlexNet</th>
      <th>VGG16</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Batch size per GPU</td>
      <td>64</td>
      <td>64</td>
      <td>64</td>
      <td>512</td>
      <td>64</td>
    </tr>
    <tr>
      <td>Optimizer</td>
      <td>sgd</td>
      <td>sgd</td>
      <td>sgd</td>
      <td>sgd</td>
      <td>sgd</td>
    </tr>
  </tbody>
</table>

<p>用于每个模型的配置。</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>variable_update</th>
      <th>local_parameter_device</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>InceptionV3</td>
      <td>parameter_server</td>
      <td>cpu</td>
    </tr>
    <tr>
      <td>ResNet50</td>
      <td>parameter_server</td>
      <td>cpu</td>
    </tr>
    <tr>
      <td>ResNet152</td>
      <td>parameter_server</td>
      <td>cpu</td>
    </tr>
    <tr>
      <td>AlexNet</td>
      <td>replicated (with NCCL)</td>
      <td>n/a</td>
    </tr>
    <tr>
      <td>VGG16</td>
      <td>replicated (with NCCL)</td>
      <td>n/a</td>
    </tr>
  </tbody>
</table>

<h3 id="结果">结果</h3>

<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:80%" src="https://www.tensorflow.org/images/perf_summary_p100_single_server.png">
</div>

<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:35%" src="https://www.tensorflow.org/images/perf_dgx1_synth_p100_single_server_scaling.png">
  <img style="width:35%" src="https://www.tensorflow.org/images/perf_dgx1_real_p100_single_server_scaling.png">
</div>

<p><strong>训练合成数据</strong></p>

<table>
  <thead>
    <tr>
      <th>GPUs</th>
      <th>InceptionV3</th>
      <th>ResNet-50</th>
      <th>ResNet-152</th>
      <th>AlexNet</th>
      <th>VGG16</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>142</td>
      <td>219</td>
      <td>91.8</td>
      <td>2987</td>
      <td>154</td>
    </tr>
    <tr>
      <td>2</td>
      <td>284</td>
      <td>422</td>
      <td>181</td>
      <td>5658</td>
      <td>295</td>
    </tr>
    <tr>
      <td>4</td>
      <td>569</td>
      <td>852</td>
      <td>356</td>
      <td>10509</td>
      <td>584</td>
    </tr>
    <tr>
      <td>8</td>
      <td>1131</td>
      <td>1734</td>
      <td>716</td>
      <td>17822</td>
      <td>1081</td>
    </tr>
  </tbody>
</table>

<p><strong>训练真实数据</strong></p>

<table>
  <thead>
    <tr>
      <th>GPUs</th>
      <th>InceptionV3</th>
      <th>ResNet-50</th>
      <th>ResNet-152</th>
      <th>AlexNet</th>
      <th>VGG16</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>142</td>
      <td>218</td>
      <td>91.4</td>
      <td>2890</td>
      <td>154</td>
    </tr>
    <tr>
      <td>2</td>
      <td>278</td>
      <td>425</td>
      <td>179</td>
      <td>4448</td>
      <td>284</td>
    </tr>
    <tr>
      <td>4</td>
      <td>551</td>
      <td>853</td>
      <td>359</td>
      <td>7105</td>
      <td>534</td>
    </tr>
    <tr>
      <td>8</td>
      <td>1079</td>
      <td>1630</td>
      <td>708</td>
      <td>N/A</td>
      <td>898</td>
    </tr>
  </tbody>
</table>

<p>从上图表可以看出，由于最大输入的限制，AlexNet 模型没有使用 8 个 GPU 来训练数据。</p>

<h3 id="其他结果">其他结果</h3>

<p>以下是批处理大小为 32 的结果。</p>

<p><strong>训练合成数据</strong></p>

<table>
  <thead>
    <tr>
      <th>GPUs</th>
      <th>InceptionV3</th>
      <th>ResNet-50</th>
      <th>ResNet-152</th>
      <th>VGG16</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>128</td>
      <td>195</td>
      <td>82.7</td>
      <td>144</td>
    </tr>
    <tr>
      <td>2</td>
      <td>259</td>
      <td>368</td>
      <td>160</td>
      <td>281</td>
    </tr>
    <tr>
      <td>4</td>
      <td>520</td>
      <td>768</td>
      <td>317</td>
      <td>549</td>
    </tr>
    <tr>
      <td>8</td>
      <td>995</td>
      <td>1485</td>
      <td>632</td>
      <td>820</td>
    </tr>
  </tbody>
</table>

<p><strong>训练真实数据</strong></p>

<table>
  <thead>
    <tr>
      <th>GPUs</th>
      <th>InceptionV3</th>
      <th>ResNet-50</th>
      <th>ResNet-152</th>
      <th>VGG16</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>130</td>
      <td>193</td>
      <td>82.4</td>
      <td>144</td>
    </tr>
    <tr>
      <td>2</td>
      <td>257</td>
      <td>369</td>
      <td>159</td>
      <td>253</td>
    </tr>
    <tr>
      <td>4</td>
      <td>507</td>
      <td>760</td>
      <td>317</td>
      <td>457</td>
    </tr>
    <tr>
      <td>8</td>
      <td>966</td>
      <td>1410</td>
      <td>609</td>
      <td>690</td>
    </tr>
  </tbody>
</table>

<h2 id="google-compute-engine-nvidia-tesla-k80-的细节">Google Compute Engine (NVIDIA® Tesla® K80) 的细节</h2>

<h3 id="环境配置-1">环境配置</h3>

<ul>
  <li>
<strong>Instance type</strong>: n1-standard-32-k80x8</li>
  <li>
<strong>GPU:</strong> 8x NVIDIA® Tesla® K80</li>
  <li>
<strong>OS:</strong> Ubuntu 16.04 LTS</li>
  <li>
<strong>CUDA / cuDNN:</strong> 8.0 / 5.1</li>
  <li>
<strong>TensorFlow GitHub hash:</strong> b1e174e</li>
  <li>
<strong>Benchmark GitHub hash:</strong> 9165a70</li>
  <li>
<strong>Build Command:</strong> <code class="highlighter-rouge">bazel build -c opt --copt=-march="haswell" --config=cuda
//tensorflow/tools/pip_package:build_pip_package</code>
</li>
  <li>
<strong>Disk:</strong> 1.7 TB Shared SSD persistent disk (800 MB/s)</li>
  <li>
<strong>DataSet:</strong> ImageNet</li>
  <li>
<strong>Test Date:</strong> May 2017</li>
</ul>

<p>每个模型的批处理大小和优化器如下表所示。除了下表所示的批处理大小，InceptionV3 和 ResNet-50 也用批处理大小为 32 进行测试。这些结果在 <em>其他结果</em> 章节。</p>

<table>
  <thead>
    <tr>
      <th>Options</th>
      <th>InceptionV3</th>
      <th>ResNet-50</th>
      <th>ResNet-152</th>
      <th>AlexNet</th>
      <th>VGG16</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Batch size per GPU</td>
      <td>64</td>
      <td>64</td>
      <td>32</td>
      <td>512</td>
      <td>32</td>
    </tr>
    <tr>
      <td>Optimizer</td>
      <td>sgd</td>
      <td>sgd</td>
      <td>sgd</td>
      <td>sgd</td>
      <td>sgd</td>
    </tr>
  </tbody>
</table>

<p>每个模型所用的配置中， variable_update 和 parameter_server 配置相同，local_parameter_device 和 cpu 配置相同。</p>

<h3 id="结果-1">结果</h3>

<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:35%" src="https://www.tensorflow.org/images/perf_gce_synth_k80_single_server_scaling.png">
  <img style="width:35%" src="https://www.tensorflow.org/images/perf_gce_real_k80_single_server_scaling.png">
</div>

<p><strong>训练合成数据</strong></p>

<table>
  <thead>
    <tr>
      <th>GPUs</th>
      <th>InceptionV3</th>
      <th>ResNet-50</th>
      <th>ResNet-152</th>
      <th>AlexNet</th>
      <th>VGG16</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>30.5</td>
      <td>51.9</td>
      <td>20.0</td>
      <td>656</td>
      <td>35.4</td>
    </tr>
    <tr>
      <td>2</td>
      <td>57.8</td>
      <td>99.0</td>
      <td>38.2</td>
      <td>1209</td>
      <td>64.8</td>
    </tr>
    <tr>
      <td>4</td>
      <td>116</td>
      <td>195</td>
      <td>75.8</td>
      <td>2328</td>
      <td>120</td>
    </tr>
    <tr>
      <td>8</td>
      <td>227</td>
      <td>387</td>
      <td>148</td>
      <td>4640</td>
      <td>234</td>
    </tr>
  </tbody>
</table>

<p><strong>训练真实数据</strong></p>

<table>
  <thead>
    <tr>
      <th>GPUs</th>
      <th>InceptionV3</th>
      <th>ResNet-50</th>
      <th>ResNet-152</th>
      <th>AlexNet</th>
      <th>VGG16</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>30.6</td>
      <td>51.2</td>
      <td>20.0</td>
      <td>639</td>
      <td>34.2</td>
    </tr>
    <tr>
      <td>2</td>
      <td>58.4</td>
      <td>98.8</td>
      <td>38.3</td>
      <td>1136</td>
      <td>62.9</td>
    </tr>
    <tr>
      <td>4</td>
      <td>115</td>
      <td>194</td>
      <td>75.4</td>
      <td>2067</td>
      <td>118</td>
    </tr>
    <tr>
      <td>8</td>
      <td>225</td>
      <td>381</td>
      <td>148</td>
      <td>4056</td>
      <td>230</td>
    </tr>
  </tbody>
</table>

<h3 id="其他结果-1">其他结果</h3>

<p><strong>训练合成数据</strong></p>

<table>
  <thead>
    <tr>
      <th>GPUs</th>
      <th>InceptionV3 (batch size 32)</th>
      <th>ResNet-50 (batch size 32)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>29.3</td>
      <td>49.5</td>
    </tr>
    <tr>
      <td>2</td>
      <td>55.0</td>
      <td>95.4</td>
    </tr>
    <tr>
      <td>4</td>
      <td>109</td>
      <td>183</td>
    </tr>
    <tr>
      <td>8</td>
      <td>216</td>
      <td>362</td>
    </tr>
  </tbody>
</table>

<p><strong>训练真实数据</strong></p>

<table>
  <thead>
    <tr>
      <th>GPUs</th>
      <th>InceptionV3 (batch size 32)</th>
      <th>ResNet-50 (batch size 32)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>29.5</td>
      <td>49.3</td>
    </tr>
    <tr>
      <td>2</td>
      <td>55.4</td>
      <td>95.3</td>
    </tr>
    <tr>
      <td>4</td>
      <td>110</td>
      <td>186</td>
    </tr>
    <tr>
      <td>8</td>
      <td>216</td>
      <td>359</td>
    </tr>
  </tbody>
</table>

<h2 id="amazon-ec2-nvidia-tesla-k80-的细节">Amazon EC2 (NVIDIA® Tesla® K80) 的细节</h2>

<h3 id="环境配置-2">环境配置</h3>

<ul>
  <li>
<strong>Instance type</strong>: p2.8xlarge</li>
  <li>
<strong>GPU:</strong> 8x NVIDIA® Tesla® K80</li>
  <li>
<strong>OS:</strong> Ubuntu 16.04 LTS</li>
  <li>
<strong>CUDA / cuDNN:</strong> 8.0 / 5.1</li>
  <li>
<strong>TensorFlow GitHub hash:</strong> b1e174e</li>
  <li>
<strong>Benchmark GitHub hash:</strong> 9165a70</li>
  <li>
<strong>Build Command:</strong> <code class="highlighter-rouge">bazel build -c opt --copt=-march="haswell" --config=cuda
//tensorflow/tools/pip_package:build_pip_package</code>
</li>
  <li>
<strong>Disk:</strong> 1TB Amazon EFS (burst 100 MiB/sec for 12 hours, continuous 50
MiB/sec)</li>
  <li>
<strong>DataSet:</strong> ImageNet</li>
  <li>
<strong>Test Date:</strong> May 2017</li>
</ul>

<p>每个模型的批处理大小和优化器如下表所示。除了下表所示的批处理大小，InceptionV3 和 ResNet-50 也用批处理大小为 32 进行测试。这些结果在 <em>其他结果</em> 章节。</p>

<table>
  <thead>
    <tr>
      <th>Options</th>
      <th>InceptionV3</th>
      <th>ResNet-50</th>
      <th>ResNet-152</th>
      <th>AlexNet</th>
      <th>VGG16</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Batch size per GPU</td>
      <td>64</td>
      <td>64</td>
      <td>32</td>
      <td>512</td>
      <td>32</td>
    </tr>
    <tr>
      <td>Optimizer</td>
      <td>sgd</td>
      <td>sgd</td>
      <td>sgd</td>
      <td>sgd</td>
      <td>sgd</td>
    </tr>
  </tbody>
</table>

<p>用于每个模型的配置。</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>variable_update</th>
      <th>local_parameter_device</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>InceptionV3</td>
      <td>parameter_server</td>
      <td>cpu</td>
    </tr>
    <tr>
      <td>ResNet-50</td>
      <td>replicated (without NCCL)</td>
      <td>gpu</td>
    </tr>
    <tr>
      <td>ResNet-152</td>
      <td>replicated (without NCCL)</td>
      <td>gpu</td>
    </tr>
    <tr>
      <td>AlexNet</td>
      <td>parameter_server</td>
      <td>gpu</td>
    </tr>
    <tr>
      <td>VGG16</td>
      <td>parameter_server</td>
      <td>gpu</td>
    </tr>
  </tbody>
</table>

<h3 id="结果-2">结果</h3>

<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:35%" src="https://www.tensorflow.org/images/perf_aws_synth_k80_single_server_scaling.png">
  <img style="width:35%" src="https://www.tensorflow.org/images/perf_aws_real_k80_single_server_scaling.png">
</div>

<p><strong>训练合成数据</strong></p>

<table>
  <thead>
    <tr>
      <th>GPUs</th>
      <th>InceptionV3</th>
      <th>ResNet-50</th>
      <th>ResNet-152</th>
      <th>AlexNet</th>
      <th>VGG16</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>30.8</td>
      <td>51.5</td>
      <td>19.7</td>
      <td>684</td>
      <td>36.3</td>
    </tr>
    <tr>
      <td>2</td>
      <td>58.7</td>
      <td>98.0</td>
      <td>37.6</td>
      <td>1244</td>
      <td>69.4</td>
    </tr>
    <tr>
      <td>4</td>
      <td>117</td>
      <td>195</td>
      <td>74.9</td>
      <td>2479</td>
      <td>141</td>
    </tr>
    <tr>
      <td>8</td>
      <td>230</td>
      <td>384</td>
      <td>149</td>
      <td>4853</td>
      <td>260</td>
    </tr>
  </tbody>
</table>

<p><strong>训练真实数据</strong></p>

<table>
  <thead>
    <tr>
      <th>GPUs</th>
      <th>InceptionV3</th>
      <th>ResNet-50</th>
      <th>ResNet-152</th>
      <th>AlexNet</th>
      <th>VGG16</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>30.5</td>
      <td>51.3</td>
      <td>19.7</td>
      <td>674</td>
      <td>36.3</td>
    </tr>
    <tr>
      <td>2</td>
      <td>59.0</td>
      <td>94.9</td>
      <td>38.2</td>
      <td>1227</td>
      <td>67.5</td>
    </tr>
    <tr>
      <td>4</td>
      <td>118</td>
      <td>188</td>
      <td>75.2</td>
      <td>2201</td>
      <td>136</td>
    </tr>
    <tr>
      <td>8</td>
      <td>228</td>
      <td>373</td>
      <td>149</td>
      <td>N/A</td>
      <td>242</td>
    </tr>
  </tbody>
</table>

<p>由于我们的 EFS 没有提供足够的吞吐量，上面的图表中我们排出了使用 8 个 GPU 来训练 AlexNet 模型的统计。</p>

<h3 id="其他结果-2">其他结果</h3>

<p><strong>训练合成数据</strong></p>

<table>
  <thead>
    <tr>
      <th>GPUs</th>
      <th>InceptionV3 (batch size 32)</th>
      <th>ResNet-50 (batch size 32)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>29.9</td>
      <td>49.0</td>
    </tr>
    <tr>
      <td>2</td>
      <td>57.5</td>
      <td>94.1</td>
    </tr>
    <tr>
      <td>4</td>
      <td>114</td>
      <td>184</td>
    </tr>
    <tr>
      <td>8</td>
      <td>216</td>
      <td>355</td>
    </tr>
  </tbody>
</table>

<p><strong>训练真实数据</strong></p>

<table>
  <thead>
    <tr>
      <th>GPUs</th>
      <th>InceptionV3 (batch size 32)</th>
      <th>ResNet-50 (batch size 32)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>30.0</td>
      <td>49.1</td>
    </tr>
    <tr>
      <td>2</td>
      <td>57.5</td>
      <td>95.1</td>
    </tr>
    <tr>
      <td>4</td>
      <td>113</td>
      <td>185</td>
    </tr>
    <tr>
      <td>8</td>
      <td>212</td>
      <td>353</td>
    </tr>
  </tbody>
</table>

<h2 id="amazon-ec2-distributed-nvidia-tesla-k80-的细节">Amazon EC2 Distributed (NVIDIA® Tesla® K80) 的细节</h2>

<h3 id="环境配置-3">环境配置</h3>

<ul>
  <li>
<strong>Instance type</strong>: p2.8xlarge</li>
  <li>
<strong>GPU:</strong> 8x NVIDIA® Tesla® K80</li>
  <li>
<strong>OS:</strong> Ubuntu 16.04 LTS</li>
  <li>
<strong>CUDA / cuDNN:</strong> 8.0 / 5.1</li>
  <li>
<strong>TensorFlow GitHub hash:</strong> b1e174e</li>
  <li>
<strong>Benchmark GitHub hash:</strong> 9165a70</li>
  <li>
<strong>Build Command:</strong> <code class="highlighter-rouge">bazel build -c opt --copt=-march="haswell" --config=cuda
//tensorflow/tools/pip_package:build_pip_package</code>
</li>
  <li>
<strong>Disk:</strong> 1.0 TB EFS (burst 100 MB/sec for 12 hours, continuous 50 MB/sec)</li>
  <li>
<strong>DataSet:</strong> ImageNet</li>
  <li>
<strong>Test Date:</strong> May 2017</li>
</ul>

<p>每个模型的批处理大小和优化器如下表所示。除了下表所示的批处理大小，InceptionV3 和 ResNet-50 也用批处理大小为 32 进行测试。这些结果在 <em>其他结果</em> 章节。</p>

<table>
  <thead>
    <tr>
      <th>Options</th>
      <th>InceptionV3</th>
      <th>ResNet-50</th>
      <th>ResNet-152</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Batch size per GPU</td>
      <td>64</td>
      <td>64</td>
      <td>32</td>
    </tr>
    <tr>
      <td>Optimizer</td>
      <td>sgd</td>
      <td>sgd</td>
      <td>sgd</td>
    </tr>
  </tbody>
</table>

<p>用于每个模型的配置。</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>variable_update</th>
      <th>local_parameter_device</th>
      <th>cross_replica_sync</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>InceptionV3</td>
      <td>distributed_replicated</td>
      <td>n/a</td>
      <td>True</td>
    </tr>
    <tr>
      <td>ResNet-50</td>
      <td>distributed_replicated</td>
      <td>n/a</td>
      <td>True</td>
    </tr>
    <tr>
      <td>ResNet-152</td>
      <td>distributed_replicated</td>
      <td>n/a</td>
      <td>True</td>
    </tr>
  </tbody>
</table>

<p>为了简化服务器设置，EC2 实例（p2.8xlarge）运行了 worker 服务器和 parameter 服务器。相同数量的 worker 服务器和 parameter 服务器使用了下述的配置：</p>

<ul>
  <li>InceptionV3: 8 instances / 6 parameter servers</li>
  <li>ResNet-50: (batch size 32) 8 instances / 4 parameter servers</li>
  <li>ResNet-152: 8 instances / 4 parameter servers</li>
</ul>

<h3 id="结果-3">结果</h3>

<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:80%" src="https://www.tensorflow.org/images/perf_summary_k80_aws_distributed.png">
</div>

<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:70%" src="https://www.tensorflow.org/images/perf_aws_synth_k80_distributed_scaling.png">
</div>

<p><strong>训练合成数据</strong></p>

<table>
  <thead>
    <tr>
      <th>GPUs</th>
      <th>InceptionV3</th>
      <th>ResNet-50</th>
      <th>ResNet-152</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>29.7</td>
      <td>52.4</td>
      <td>19.4</td>
    </tr>
    <tr>
      <td>8</td>
      <td>229</td>
      <td>378</td>
      <td>146</td>
    </tr>
    <tr>
      <td>16</td>
      <td>459</td>
      <td>751</td>
      <td>291</td>
    </tr>
    <tr>
      <td>32</td>
      <td>902</td>
      <td>1388</td>
      <td>565</td>
    </tr>
    <tr>
      <td>64</td>
      <td>1783</td>
      <td>2744</td>
      <td>981</td>
    </tr>
  </tbody>
</table>

<h3 id="其他结果-3">其他结果</h3>

<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:50%" src="https://www.tensorflow.org/images/perf_aws_synth_k80_multi_server_batch32.png">
</div>

<p><strong>训练合成数据</strong></p>

<table>
  <thead>
    <tr>
      <th>GPUs</th>
      <th>InceptionV3 (batch size 32)</th>
      <th>ResNet-50 (batch size 32)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>29.2</td>
      <td>48.4</td>
    </tr>
    <tr>
      <td>8</td>
      <td>219</td>
      <td>333</td>
    </tr>
    <tr>
      <td>16</td>
      <td>427</td>
      <td>667</td>
    </tr>
    <tr>
      <td>32</td>
      <td>820</td>
      <td>1180</td>
    </tr>
    <tr>
      <td>64</td>
      <td>1608</td>
      <td>2315</td>
    </tr>
  </tbody>
</table>

<h2 id="方法">方法</h2>

<p>上述结果是使用该 <a href="https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks">脚本</a> 运行在各种平台上而生成。<a href="https://www.tensorflow.org/performance/performance_models">《High-Performance Models》</a> 文章详细描述了脚本中的技术，以及如何执行脚本的示例。</p>

<p>为了创建尽可能重复的结果，每个测试运行 5 次，然后取平均值。在给定的平台上，GPU 是在默认状态下运行的。对于 NVIDIA® Tesla® K80 来说这意味着不使用 <a href="https://devblogs.nvidia.com/parallelforall/increase-performance-gpu-boost-k80-autoboost/">GPU
Boost</a>。</p>

<p>对于每个测试，需要完成 10 次预热，然后再平均完成 100 次测试。</p>

<blockquote>
  <ul>
    <li>原文地址：<a href="https://www.tensorflow.org/performance/benchmarks">https://www.tensorflow.org/performance/benchmarks</a>
</li>
    <li>译文出自：<a href="https://github.com/xitu/gold-miner">掘金翻译计划</a>
</li>
    <li>译者：<a href="https://github.com/charsdavy">charsdavy</a>
</li>
    <li>校对者：<a href="https://github.com/joyking7">joyking7</a>
</li>
  </ul>
</blockquote>

<blockquote>
  <p><a href="https://github.com/xitu/gold-miner">掘金翻译计划</a> 是一个翻译优质互联网技术文章的社区，文章来源为 <a href="https://juejin.im">掘金</a> 上的英文分享文章。内容覆盖 <a href="https://github.com/xitu/gold-miner#android">Android</a>、<a href="https://github.com/xitu/gold-miner#ios">iOS</a>、<a href="https://github.com/xitu/gold-miner#%E5%89%8D%E7%AB%AF">前端</a>、<a href="https://github.com/xitu/gold-miner#%E5%90%8E%E7%AB%AF">后端</a>、<a href="https://github.com/xitu/gold-miner#%E5%8C%BA%E5%9D%97%E9%93%BE">区块链</a>、<a href="https://github.com/xitu/gold-miner#%E4%BA%A7%E5%93%81">产品</a>、<a href="https://github.com/xitu/gold-miner#%E8%AE%BE%E8%AE%A1">设计</a>、<a href="https://github.com/xitu/gold-miner#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD">人工智能</a>等领域，想要查看更多优质译文请持续关注 <a href="https://github.com/xitu/gold-miner">掘金翻译计划</a>、<a href="http://weibo.com/juejinfanyi">官方微博</a>、<a href="https://zhuanlan.zhihu.com/juejinfanyi">知乎专栏</a>。</p>
</blockquote>


            
            
            
            

            <!-- Tag Buttons -->
            <ul class="actions">
              
              
              <li><a href="/tags/#tensorflow" class="button small"># tensorflow</a></li>
              
              
              
              <li><a href="/tags/#%E6%80%A7%E8%83%BD" class="button small"># 性能</a></li>
              
              
              
              <li><a href="/tags/#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0" class="button small"># 机器学习</a></li>
              
                  
            </ul>
          </section>

          <div class="comments-wrapper">
          <!--
          <div id="disqus_thread"></div>
          <script>
              /**
               *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
               *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
               */

              var disqus_config = function () {
                  this.page.url = '/blog/tensorflow-performance-benchmarks/';  /*Replace PAGE_URL with your page's canonical URL variable*/
                  this.page.identifier = '/blog/tensorflow-performance-benchmarks/'; /*Replace PAGE_IDENTIFIER with your page's unique identifier variable*/
              };

              (function() {  /* dont endit below this line */
                  var d = document, s = d.createElement('script');

                  s.src = 'https://.disqus.com/embed.js';

                  s.setAttribute('data-timestamp', +new Date());
                  (d.head || d.body).appendChild(s);
              })();
          </script>
          <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>-->

          <!-- 来必力City版安装代码 -->
          <div id="lv-container" data-id="city" data-uid="MTAyMC8zMzY4My8xMDIzOA==">
          <script type="text/javascript">
             (function(d, s) {
                 var j, e = d.getElementsByTagName(s)[0];

                 if (typeof LivereTower === 'function') { return; }

                 j = d.createElement(s);
                 j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
                 j.async = true;

                 e.parentNode.insertBefore(j, e);
             })(document, 'script');
          </script>
          <noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
          </div>
        </div>
        <!-- /.comments-wrapper -->



					<!-- Footer -->
						<footer>
              <ul class="actions">
                
                  <li><a href="/blog/personal-projects/" class="button special icon fa-arrow-left"></a></li>
                
                <li><a href="/blog/" class="button">All Blog</a></li>
                
                  <li><a href="/blog/cocoapods-source-commit/" class="button special icon fa-arrow-right"></a></li>
                
              </ul>
						</footer>
					</div>

				<!-- Footer -->
        <footer id="footer">
  <section>
    <form method="POST" action="https://formspree.io/chars.davy@gmail.com">
      <div class="field">
        <label for="name">Name</label>
        <input type="text" name="name" id="name">
      </div>
      <div class="field">
        <label for="email">Email</label>
        <input type="text" name="email" id="email">
      </div>
      <div class="field">
        <label for="message">Message</label>
        <textarea name="message" id="message" rows="3"></textarea>
      </div>
      <ul class="actions">
        <li><input type="submit" value="Send Message"></li>
      </ul>
    </form>
  </section>
  <section class="split contact">
    <section class="alt">
      <h3>Location</h3>
      <p>Guangzhou, China</p>
    </section>
    <section>
      <h3>QQ</h3>
      <p><a href="qq:124808738">124808738</a></p>
    </section>
    <section>
      <h3>Email</h3>
      <p><a href="mailto:chars.davy@gmail.com">chars.davy@gmail.com</a></p>
    </section>
    <section>
      <h3>Social</h3>
      <ul class="icons alt">
        <li><a href="https://twitter.com/charsdavy" class="icon fa-twitter" rel="nofollow"><span class="label">Twitter</span></a></li>
        <li><a href="https://facebook.com/wei.deng.1460" class="icon fa-facebook" rel="nofollow"><span class="label">Facebook</span></a></li>
        <li><a href="https://instagram.com/chars.davy" class="icon fa-instagram" rel="nofollow"><span class="label">Instagram</span></a></li>
        <li><a href="https://github.com/charsdavy" class="icon fa-github" rel="nofollow"><span class="label">GitHub</span></a></li>
      </ul>
    </section>
  </section>
</footer>
<div id="back-top" style="position:fixed;bottom:40px;right:30px;cursor: pointer;">
  <a href="#top" title="返回顶部"><img src="/images/arrow-up.png"></a>
</div>
<script type="text/javascript">
  $("#back-top").hide();
  $(document).ready(function () {
    $(window).scroll(function () {
      if ($(this).scrollTop() > 800) {
        $('#back-top').fadeIn();
      } else {
        $('#back-top').fadeOut();
      }
    });
    $('#back-top a').click(function () {
      $('body,html').animate({
        scrollTop: 0
      }, 1200);
      return false;
    });
  });
</script>
<!-- Copyright -->
<div id="copyright">
  <ul>
<li>2016 - 2018 © Chars</li>
<li>Design by <a href="https://html5up.net" rel="nofollow">HTML5 UP</a>
</li>
<li>Jekyll Integration by <a href="https://soundgrail.com">SoundGrail</a>
</li>
</ul>
</div>


			</div>

      <!-- Scripts -->
  		<!-- DYN -->
<script src="/assets/js/jquery.min.js"></script>
<script src="/assets/js/jquery.scrollex.min.js"></script>
<script src="/assets/js/jquery.scrolly.min.js"></script>
<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<script src="/assets/js/main.js"></script>

			<script async src="https://www.googletagmanager.com/gtag/js?id=UA-80507808-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments)};
  gtag('js', new Date());

  gtag('config', 'UA-80507808-1');
</script>


	</body>
</html>
